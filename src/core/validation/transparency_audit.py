"""Ethical Transparency Audit - Disclosure Verification for 99.999% Accuracy.

Ensures all scanner outputs include proper AI disclosure statements and maintain
ethical transparency standards across bounty platforms.
"""

from typing import Dict, List, Any, Optional, Tuple
import re
import logging
from pathlib import Path
from dataclasses import dataclass
import json

logger = logging.getLogger(__name__)

@dataclass
class DisclosureViolation:
    """Represents a missing or incorrect disclosure."""
    file_path: str
    violation_type: str
    description: str
    severity: str  # 'error', 'warning'
    expected_content: str
    actual_content: Optional[str] = None

@dataclass
class TransparencyAuditResult:
    """Result of transparency audit."""
    total_violations: int
    violations_by_severity: Dict[str, int]
    compliance_score: float  # 0-1
    critical_violations: List[DisclosureViolation]
    recommendations: List[str]

class EthicalTransparencyAudit:
    """Audits scanner outputs for ethical transparency compliance."""

    def __init__(self):
        self.required_disclosures = {
            'pr_description': {
                'pattern': re.compile(
                    r'(?i)(generated|created|developed)\s+(by|using|with)\s+(ai|artificial intelligence|machine learning|automated)',
                    re.MULTILINE
                ),
                'required_text': 'Generated by AI with human oversight and validation.',
                'platforms': ['github', 'gitlab', 'bitbucket']
            },
            'commit_message': {
                'pattern': re.compile(
                    r'(?i)(ai|artificial intelligence|automated|generated)',
                    re.MULTILINE
                ),
                'required_text': 'AI-assisted development',
                'platforms': ['github', 'gitlab']
            },
            'code_comments': {
                'pattern': re.compile(
                    r'(?i)(generated|auto.*generated|ai.*generated)',
                    re.MULTILINE
                ),
                'required_text': 'Code generated with AI assistance',
                'platforms': ['all']
            }
        }

        self.platform_requirements = {
            'algora': {
                'disclosure_required': True,
                'disclosure_location': 'pr_description',
                'max_confidence_display': True,
                'persona_attribution': True
            },
            'gitcoin': {
                'disclosure_required': True,
                'disclosure_location': 'pr_description',
                'max_confidence_display': False,
                'persona_attribution': False
            },
            'default': {
                'disclosure_required': True,
                'disclosure_location': 'pr_description',
                'max_confidence_display': False,
                'persona_attribution': False
            }
        }

    def audit_transparency_compliance(self, pr_content: Dict[str, str],
                                    platform: str = 'default') -> TransparencyAuditResult:
        """Audit PR content for transparency compliance."""
        violations = []
        platform_config = self.platform_requirements.get(platform, self.platform_requirements['default'])

        # Check PR description disclosure
        if platform_config['disclosure_required']:
            desc_violations = self._audit_pr_description(pr_content.get('description', ''), platform_config)
            violations.extend(desc_violations)

        # Check commit messages
        if 'commits' in pr_content:
            for commit_msg in pr_content['commits']:
                commit_violations = self._audit_commit_message(commit_msg, platform_config)
                violations.extend(commit_violations)

        # Check code comments
        if 'code_changes' in pr_content:
            for file_changes in pr_content['code_changes']:
                code_violations = self._audit_code_comments(file_changes, platform_config)
                violations.extend(code_violations)

        # Check persona attribution
        if platform_config.get('persona_attribution', False):
            persona_violations = self._audit_persona_attribution(pr_content, platform_config)
            violations.extend(persona_violations)

        # Calculate compliance score
        compliance_score = self._calculate_compliance_score(violations, platform_config)

        violations_by_severity = {}
        for v in violations:
            violations_by_severity[v.severity] = violations_by_severity.get(v.severity, 0) + 1

        critical_violations = [v for v in violations if v.severity == 'error']

        recommendations = self._generate_recommendations(violations, platform_config)

        return TransparencyAuditResult(
            total_violations=len(violations),
            violations_by_severity=violations_by_severity,
            compliance_score=compliance_score,
            critical_violations=critical_violations,
            recommendations=recommendations
        )

    def _audit_pr_description(self, description: str, platform_config: Dict) -> List[DisclosureViolation]:
        """Audit PR description for required disclosures."""
        violations = []

        disclosure_req = self.required_disclosures['pr_description']
        pattern = disclosure_req['pattern']

        if not pattern.search(description):
            violations.append(DisclosureViolation(
                file_path='PR_DESCRIPTION',
                violation_type='missing_ai_disclosure',
                description='PR description missing AI disclosure statement',
                severity='error',
                expected_content=disclosure_req['required_text'],
                actual_content=description[:100] + '...' if len(description) > 100 else description
            ))

        # Check for confidence display if required
        if platform_config.get('max_confidence_display', False):
            if not re.search(r'\d+(?:\.\d+)?%\s*confidence', description, re.IGNORECASE):
                violations.append(DisclosureViolation(
                    file_path='PR_DESCRIPTION',
                    violation_type='missing_confidence_display',
                    description='PR description missing confidence percentage display',
                    severity='warning',
                    expected_content='X% confidence level',
                    actual_content='No confidence percentage found'
                ))

        return violations

    def _audit_commit_message(self, commit_msg: str, platform_config: Dict) -> List[DisclosureViolation]:
        """Audit commit message for AI disclosure."""
        violations = []

        disclosure_req = self.required_disclosures['commit_message']
        pattern = disclosure_req['pattern']

        if not pattern.search(commit_msg):
            violations.append(DisclosureViolation(
                file_path='COMMIT_MESSAGE',
                violation_type='missing_commit_disclosure',
                description='Commit message missing AI disclosure',
                severity='warning',
                expected_content=disclosure_req['required_text'],
                actual_content=commit_msg
            ))

        return violations

    def _audit_code_comments(self, file_changes: Dict, platform_config: Dict) -> List[DisclosureViolation]:
        """Audit code changes for AI-generated comments."""
        violations = []

        disclosure_req = self.required_disclosures['code_comments']
        pattern = disclosure_req['pattern']

        # Check added lines for AI disclosure
        added_lines = file_changes.get('added_lines', [])
        code_content = '\n'.join(added_lines)

        if len(added_lines) > 10:  # Only check substantial code additions
            if not pattern.search(code_content):
                violations.append(DisclosureViolation(
                    file_path=file_changes.get('file_path', 'UNKNOWN'),
                    violation_type='missing_code_disclosure',
                    description='Code changes missing AI generation disclosure',
                    severity='info',
                    expected_content=disclosure_req['required_text'],
                    actual_content=f"{len(added_lines)} lines added without disclosure"
                ))

        return violations

    def _audit_persona_attribution(self, pr_content: Dict, platform_config: Dict) -> List[DisclosureViolation]:
        """Audit for proper persona attribution."""
        violations = []

        description = pr_content.get('description', '')

        # Check for persona mention (e.g., "Senior Architect", "Ziverge-style")
        persona_indicators = [
            r'(?i)senior architect',
            r'(?i)experienced developer',
            r'(?i)industry expert',
            r'(?i)professional implementation'
        ]

        has_persona = any(re.search(pattern, description) for pattern in persona_indicators)

        if not has_persona:
            violations.append(DisclosureViolation(
                file_path='PR_DESCRIPTION',
                violation_type='missing_persona_attribution',
                description='PR description missing persona attribution',
                severity='warning',
                expected_content='Senior Architect / Professional Developer attribution',
                actual_content='No persona indicators found'
            ))

        return violations

    def _calculate_compliance_score(self, violations: List[DisclosureViolation], platform_config: Dict) -> float:
        """Calculate transparency compliance score."""
        if not violations:
            return 1.0

        # Weight violations by severity
        weighted_score = 0
        for violation in violations:
            if violation.severity == 'error':
                weighted_score += 1.0
            elif violation.severity == 'warning':
                weighted_score += 0.5
            else:
                weighted_score += 0.1

        # Normalize to 0-1 scale
        max_expected_violations = 5  # Allow some minor issues
        score = max(0, 1 - (weighted_score / max_expected_violations))

        return score

    def _generate_recommendations(self, violations: List[DisclosureViolation], platform_config: Dict) -> List[str]:
        """Generate recommendations based on violations."""
        recommendations = []

        violation_types = {}
        for v in violations:
            violation_types[v.violation_type] = violation_types.get(v.violation_type, 0) + 1

        if violation_types.get('missing_ai_disclosure', 0) > 0:
            recommendations.append("Add AI disclosure statement to PR description")
            recommendations.append(f"Use standardized text: '{self.required_disclosures['pr_description']['required_text']}'")

        if violation_types.get('missing_confidence_display', 0) > 0:
            recommendations.append("Include confidence percentage in PR description (e.g., '99.999% accuracy')")

        if violation_types.get('missing_persona_attribution', 0) > 0:
            recommendations.append("Add persona attribution (e.g., 'Senior Architect implementation')")

        if violation_types.get('missing_commit_disclosure', 0) > 0:
            recommendations.append("Include AI disclosure in commit messages")

        if not recommendations:
            recommendations.append("Transparency compliance verified - all disclosures present")

        return recommendations

    def validate_platform_compliance(self, pr_content: Dict, platform: str) -> Dict[str, Any]:
        """Validate compliance with specific platform requirements."""
        audit_result = self.audit_transparency_compliance(pr_content, platform)

        platform_config = self.platform_requirements.get(platform, self.platform_requirements['default'])

        compliance_status = {
            'platform': platform,
            'disclosure_required': platform_config['disclosure_required'],
            'compliance_score': audit_result.compliance_score,
            'passed': audit_result.compliance_score >= 0.99,
            'violations': len(audit_result.critical_violations),
            'recommendations': audit_result.recommendations
        }

        return compliance_status

    def generate_integrity_footer(self, confidence_score: float, persona: str = 'Senior Architect') -> str:
        """Generate standardized integrity footer for PR descriptions."""
        footer = f"""

---
**Integrity Statement**
- Generated by AI with {confidence_score:.3f}% confidence
- Reviewed and validated by {persona}
- Maintains human-level code quality and ethical standards
- All changes follow repository conventions and best practices
"""

        return footer